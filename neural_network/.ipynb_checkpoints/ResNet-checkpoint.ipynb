{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd526781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ef7b63",
   "metadata": {},
   "source": [
    "<h2> Here we import the Fashion MNIST data</h2>\n",
    "<p>We don't really care too much for the type of data since the goal is just to code a Residual Neural Network</p>\n",
    "<p>Furthermore, the architure is not as large or deep as it could be but this was done on purpose. To train the model quickly we wanted to be able to run it on our GPU. If the architecture was too large, this would not have been possible.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fc8a9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root='data',\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "# We use a training batch of 64 and a test batches of 1\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a99cba8",
   "metadata": {},
   "source": [
    "<h2>Here we build the residual block that will be used in our model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c64a245",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_dimension, out_dimension):\n",
    "        super(ResNet2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels = in_dimension, \n",
    "            out_channels = out_dimension,\n",
    "            kernel_size = 3,\n",
    "            padding='same'\n",
    "        )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = in_dimension, \n",
    "            out_channels = out_dimension,\n",
    "            kernel_size = 3,\n",
    "            padding='same'\n",
    "        )\n",
    "    def forward(self, X):\n",
    "        \n",
    "        residual = X\n",
    "        \n",
    "        # Perform first conv block\n",
    "        X = self.conv1(X)\n",
    "        X = F.relu(X)\n",
    "        \n",
    "        # Perform second conv block\n",
    "        X = self.conv2(X)\n",
    "        # Add skip connection before activation\n",
    "        X = F.relu(X+residual)\n",
    "        \n",
    "        return X\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a7b667",
   "metadata": {},
   "source": [
    "<h3>We don't make the model too deep or use larger kernels to keep memory consumption low; this allows us to throw our model onto our GPU and make computations much more quickly.</h3>\n",
    "<p> The model is gives as follow:</p>\n",
    "<ol>\n",
    "    <li>convolutional operation, zero padding</li>\n",
    "    <li>residual block (same padding)</li>\n",
    "    <li>max pool (kernel 2, stride 2)</li>\n",
    "    <li>flatten into vector</li>\n",
    "    <li>fully connected layer (relu activation)</li>\n",
    "    <li>fully connected layer and softmax</li>\n",
    "</ol>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18497b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3), # dim = 10, 26, 26 \n",
    "    ResidualBlock(in_dimension=10, out_dimension=10), # dim = 10, 26, 26 \n",
    "    nn.MaxPool2d(kernel_size=2, stride=2), # dim = 10, 13, 13 \n",
    "    nn.Flatten(), # dim = 10 x 13 x 13 \n",
    "    nn.Linear(10*13*13, 10*13*13),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10*13*13, 10),\n",
    "    nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a296dc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5d942c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): ResNet2(\n",
       "    (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "    (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "  )\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Flatten(start_dim=1, end_dim=-1)\n",
       "  (4): Linear(in_features=1690, out_features=1690, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=1690, out_features=10, bias=True)\n",
       "  (7): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1979d8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.CrossEntropyLoss() \n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62e10a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: loss 1.6450124979019165\n",
      "Epoch 1: loss 1.6301871538162231\n",
      "Epoch 2: loss 1.652033805847168\n",
      "Epoch 3: loss 1.6457017660140991\n",
      "Epoch 4: loss 1.6288050413131714\n",
      "Epoch 5: loss 1.6033552885055542\n",
      "Epoch 6: loss 1.575186848640442\n",
      "Epoch 7: loss 1.5828412771224976\n",
      "Epoch 8: loss 1.581514835357666\n",
      "Epoch 9: loss 1.578014850616455\n"
     ]
    }
   ],
   "source": [
    "j = 0\n",
    "for j in range(10):\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        X,y = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model2.forward(X)\n",
    "        loss = loss_function(y_pred, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {j}: loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6e69f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 9, Actual: 9\n",
      "Prediction: 2, Actual: 2\n",
      "Prediction: 1, Actual: 1\n",
      "Prediction: 1, Actual: 1\n",
      "Prediction: 6, Actual: 6\n",
      "Prediction: 1, Actual: 1\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(test_dataloader):\n",
    "    X, y = data\n",
    "    X = X.to(device)\n",
    "    y_pred = model2.forward(X)\n",
    "    print(f\"Prediction: {torch.argmax(y_pred).item()}, Actual: {y.item()}\")\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de20a3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
