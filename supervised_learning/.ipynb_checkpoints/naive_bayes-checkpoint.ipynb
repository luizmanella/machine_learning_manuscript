{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a0cbcd",
   "metadata": {},
   "source": [
    "<h1>Naive Bayes Classifier</h1>\n",
    "<h3>Goal:</h3>\n",
    "<p>Here we will use the Gaussian naive Bayes classifier, imported from sklearn, over census data to predict if a person makes above $50k a year. The data is brought in from Keras.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "17273694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08683a6e",
   "metadata": {},
   "source": [
    "<p>We need to update the column headers of the dataframe</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d82b17e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('adult.csv', header=None)\n",
    "column_names = ['age',\n",
    "                'workclass',\n",
    "                'fnlwgt', \n",
    "                'education', \n",
    "                'education_num', \n",
    "                'marital_status', \n",
    "                'occupation',\n",
    "                'relationship',\n",
    "                'race',\n",
    "                'sex',\n",
    "                'capital_gain', \n",
    "                'capital_loss', \n",
    "                'hours_per_week', \n",
    "                'native_country', \n",
    "                'income']\n",
    "df.columns = column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8f51cc",
   "metadata": {},
   "source": [
    "<p>We need to do a quick check over the columns for their data type. If we find categorical data, we will perform One-Hot-Encoding</p>\n",
    "<p>We also need to check for null values and act accordingly</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "0abfb20c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                int64\n",
       "workclass         object\n",
       "fnlwgt             int64\n",
       "education         object\n",
       "education_num      int64\n",
       "marital_status    object\n",
       "occupation        object\n",
       "relationship      object\n",
       "race              object\n",
       "sex               object\n",
       "capital_gain       int64\n",
       "capital_loss       int64\n",
       "hours_per_week     int64\n",
       "native_country    object\n",
       "income            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "66f04956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education_num     0\n",
       "marital_status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital_gain      0\n",
       "capital_loss      0\n",
       "hours_per_week    0\n",
       "native_country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed1133c",
   "metadata": {},
   "source": [
    "<p>A quick look into the data shows there are some empty fields which contain input ' ?'</p>\n",
    "<p>We replace with those question marks with np.NaN and do a dropna. I do not think imputing is a good idea here. Imputing could introduce bias which would be counterproductive.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "787ca13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['workclass'].replace(' ?', np.NaN, inplace=True)\n",
    "df['occupation'].replace(' ?', np.NaN, inplace=True)\n",
    "df['native_country'].replace(' ?', np.NaN, inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e88e350e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age               0\n",
       "workclass         0\n",
       "fnlwgt            0\n",
       "education         0\n",
       "education_num     0\n",
       "marital_status    0\n",
       "occupation        0\n",
       "relationship      0\n",
       "race              0\n",
       "sex               0\n",
       "capital_gain      0\n",
       "capital_loss      0\n",
       "hours_per_week    0\n",
       "native_country    0\n",
       "income            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851e3498",
   "metadata": {},
   "source": [
    "<p>Next we split the data and perform one-hot-encoding over the categorical data. We must also transform the labels into something feasible (i.e., 0 and 1)</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "5f58856c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luizm\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:3636: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  self[k1] = value[k2]\n",
      "C:\\Users\\luizm\\AppData\\Local\\Temp/ipykernel_41488/2838246600.py:19: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  encoded_df['income'] = y\n"
     ]
    }
   ],
   "source": [
    "# We split the data into label and features.\n",
    "x = df.drop(['income'], axis=1)\n",
    "y = df['income']\n",
    "y = np.array(preprocessing.LabelEncoder().fit_transform(y))\n",
    "\n",
    "# We separate the integer type data and categorical data so we can one-hot-encode the categorical\n",
    "int_type = list(x.select_dtypes(include=[np.int64]).columns)\n",
    "categorical = list(x.select_dtypes(include=[object]).columns)\n",
    "encoded_df = pd.DataFrame()\n",
    "for c in categorical:\n",
    "    u = x[c].unique().tolist()\n",
    "    new_names = [f\"{c}_{i}\" for i in u]\n",
    "    ehc = preprocessing.OneHotEncoder(sparse=False).fit(x[c].to_numpy().reshape(-1,1))\n",
    "    transformed = ehc.transform(x[c].to_numpy().reshape(-1,1))\n",
    "    encoded_df[new_names] = transformed\n",
    "\n",
    "# Join the data to properly maintain row structure when dropping nulls\n",
    "encoded_df[int_type] = x[int_type]\n",
    "encoded_df['income'] = y\n",
    "encoded_df.dropna(inplace=True)\n",
    "\n",
    "# Reseparate the data and perform a split\n",
    "x = encoded_df.drop(['income'], axis=1)\n",
    "y = encoded_df['income']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8732bf",
   "metadata": {},
   "source": [
    "<h3>Model</h3>\n",
    "<p>Next we build the model, fit the data, make predictions, and lastly check the accuracy of our prediction.</p>\n",
    "<p>We could go further and check the precision, recall, and F1 score, although this is merely a simple toy project to implement naive Bayes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4a9863cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "model.fit(x_train, y_train)\n",
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30e7528",
   "metadata": {},
   "source": [
    "<p>The accuracy is not amazing but also isn't horrible. It is possible the choice of Gaussian naive Bayes is fundamentally wrong and the data is distributed differently.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "8a257440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7428162632645762"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = accuracy_score(y_test, pred)\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
